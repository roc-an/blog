# 01-课程简介及算法分析

本套课程分为两大主题，第一块主题是**算法分析**，第二块是**算法设计**。

在开始设计算法之前，我们不得不掌握一些分析算法的实用技巧，这样才能知道哪一种算法方式更有效。

## （一）算法分析

**算法分析属于理论研究，是关于计算程序性能和资源利用的研究，这里尤其关注性能。**

我们要学习如何让计算机程序变得更快，我们也将涉及并讨论其他问题，例如通信、存储器（不管是内存或是磁盘存储）。

Q：思考一个问题，如果你在写一段程序，有什么是比性能更重要的呢？
A：正确性、简洁、可维护性、程序员的时间成本、稳定性、软件所拥有的功能特性、模块化等等。还有一个点也非常重要，那就是安全性（2000 年以后，安全变得比性能更加重要）。

Q（追问）：还有什么是比性能更重要的？为什么人们更青睐苹果操作系统，而不是 Windows，这些人简直成了苹果的教徒？
A：用户体验友好。如果你一直关注计算机的发展历程，你就会见证 90 年代计算机是如何进入用户友好时代。从几乎空白的状态演进到了目前友好的用户体验。因此，以上这些所有事情，都要比性能重要。

### RAM 模型

分析算法的结果意味着预测算法需要的资源。

通常我们度量的是计算时间。在能够分析一个算法之前，我们必须有一个要使用的实现技术的「模型」，包括描述所用资源及其代价的模型。本系列文章中，我们假定一种通用的单处理器计算模型——随机访问机（random-access machine，RAM）来作为我们的实现技术，算法可以用计算机程序来实现。在 RAM 模型中，指令一条接一条地执行，没有并发操作。

RAM 模型包含真实计算机中常见的指令：

* 算术指令：
  * 加法
  * 减法
  * 乘法
  * 除法
  * 取余
  * 向下取整
  * 向上取整
* 数据移动指令：
  * 装入
  * 存储
  * 复制
* 控制指令：
  * 条件与无条件转移
  * 子程序调用与返回

每条这样的指令所需时间都为常量。

采用 RAM 模型即使分析一个简单的算法也可能是一个挑战。需要的数学工具可能包括组合学、概率论、代数技巧，以及识别一个公式中最有意义的项的能力。

### 为什么要学习算法？

这是一门讨论性能的课，如果算法和性能都不重要，为什么还要学习它们？

这里有几点重要的原因：

1.通常，**性能的好与坏，直接决定着可行还是不可行。**例如，对于实时的需求，如果程序不快，这只能表示它不可行。或者，如果它占用过多的内存，也只能说是不可行。

所以，算法总是处于解决问题的最前沿。如果你讨论的只是重新实现十年前人们就做过的东西，那某种意义上，性能就不再重要。但是如果你正在积极尝试其他人没有成功的事情，之所以别人没有成功，因为太消耗时间、不具备可扩展的空间。这是第一层原因，**算法能够将不可行变为可行（feasible versus infeasible）**。

2.第二层原因，**算法是一种描述程序行为的语言**。它逐渐成为一种广泛应用于计算机科学领域、已经被所有的实践者所采用的理论语言，它是一种让程序最为简洁的思考方式。我们有一个很好的比喻来形容性能，以及为何性能处于最底层，它所扮演的角色就如同经济中的货币一般。

想想一沓一百美元的钞票有什么好的？你也许更需要有食物、水、房子等等，你需要用钞票才能......（前提是你有的话），才能买下这些商品。尽管水对于你的生命比钞票重要。同样，性能是确保良好的用户体验的前提，也是安全的保障。

你可能听别人说到，我希望有更多的功能，因此人们会用 Java 来写程序，尽管它比 C 写的程序要慢很多，有人说用 Java 编程性能会损失三倍左右，但是值得为 Java 付出这些代价，因为它提供面向对象的特性以及异常机制等等。所以人们能够接受三倍的性能损失。

这就是我们为什么需要性能，因为需要性能作为支付其他东西的“货币”，因为它作为衡量一个程序的一般性标准，你会考虑你愿意消耗 2 倍的性能在这块上面，还是消耗 3 倍的性能在安全上面，等等。

3.我们学习算法的最后一层原因，**这里充满了兴趣**。速度永远让人渴望，对吗？为什么人们喜欢驾驶跑车、赛马、滑雪这些快速的东西？还比如火箭，为什么？因为我们向往速度。

以上，这是为什么学习算法的一些基本概念，这些是开展后续课程的基本前提。我们希望知道如何才能让计算转化为金钱。

## （二）排序问题-插入排序

让我们从一个非常简单的问题开始，这是一个在算法学习中最古老的问题。就是**排序问题**。排序包含了很多基本算法，我们将用几个课时来介绍它。

举一个排序的例子：

输入：`n` 个数的一组序列 `<a1, a2, ..., an>`
输出：输入序列的一个排列 `<a1', a2', ..., an'>`，满足 `a1' <= a2' <= ... <= an'`

排序后每个数字出现且仅出现一次，让它们由小到大依次递增。

我们先用一段伪代码来描述插入排序（Insertion-Sort），伪代码能够让我们更容易理解算法所要表达的意思：

注意，我们将伪代码的过程命名为 `INSERTION-SORT`，参数是一个数组 `A[1..n]`，包含着 `n` 个要排序的数。`A` 中元素的数目用 `A.length` 表示。另外和 JS 不同，伪代码中数组元素是从索引 1 开始的，而不是 JS 中的 0。

```
INSERTION-SORT(A)
  for j = 2 to A.length
    key = A[j]

    // 将 A[j] 插入到排序后数组 A[1..j - 1] 中
    i = j - 1
    while i > 0 and A[i] > key
      A[i + 1] = A[i]
      i = i - 1
    A[i+1] = key
```

要想第一次就搞懂上面的步骤，还是有挑战性的，这里给大家放出来书中的例子，可以对照着伪代码理一遍就有思路了：`A = <5, 2, 4, 6, 1, 3>`。

### 插入排序就好比给手中的扑克牌排序

其实这个算法的场景在生活中人们大多都经历过。就是**给扑克牌排序**。如图：

可以将整个过程比作给手里的一把扑克牌从左至右、从小到大依次排序，那么：

```
INSERTION-SORT(A)
  for j = 2 to A.length // 一开始从左数第 2 张牌开始
    key = A[j] // 将当前要比较的牌抽出来，开始往左看，思考要放在哪
    i = j - 1 // 先比较前一张牌

    while i > 0 and A[i] > key // 如果前一张还有牌（就是 i > 0），并且前一张牌还比当前抽出的牌点大
      A[i + 1] = A[i] // 那就把前一张牌后移
      i = i - 1 // 之后继续比较再前一张牌。以此类推，直到比较到前面没牌，或者前面牌的点数小于等于抽出牌点数时停止
    A[i+1] = key // 最后再将抽出的牌插入到相应的位置（即最终比较的前一张牌的后面），排序结束。
```

### 插入排序的核心思想

搞懂代码执行流程后，我们进一步分析这其中的思想。

这段排序算法，有**内外两层循环**：

* 外层循环，循环条件是 `j` 从 2 递增到数组长度 `n`；
* 内层循环，循环开始于 `j - 1`，并递减至 0，或者递减到前一个数不再比当前的 `key` 变量存着的数大。

再深入循环内部。如图：

在每个外层循环中，先找到 `j` 这个位置，把数组的 `j` 这个位置的数字提取出来，称之为键值 `key`。

接下来这点很重要。这段算法中**维护着一个「循环不变式（invariant）」，每次循环都将更新这个不变式。**这个不变式就是数组已经排序过的部分，每次循环的目的是完成增量（使已排序部分的长度 +1）。

该算法中循环中实现增量的方法是：提取位置为 `j` 的数字作为 `key` 值，然后一步步地把前面的值拷贝到其下一位上，直到找到 `key` 合适的插入位置，然后我们再插入 `key` 值，这就是为什么叫**插入排序**的原因。

接着，当数组 `A` 中的数字从 1 到 `j` 的位置已经排序好，那么接下来会执行 `j + 1`，然后以此类推，再进入到下一次外层循环，直到最终排序完成。

### 数组 8, 2, 4, 9, 3, 6 的插入排序过程

举个例子，数组 `A = <8, 2, 4, 9, 3, 6>`，的插入排序过程如图：

可以看到，每一步（图中的每一行）都是将 `j` 位置的数提取为 `key`，然后前面的数依次后移，直到在合适的位置插入 `key`，直到最终插入排序结束。

## （三）伪代码约定

上面的例子中我们一开始使用了伪代码。伪代码很适合描述算法，因为它足够简洁。《算法导论》的后续章节中也离不开各种伪代码示例。这里单独抽出一小节，来明确在《算法导论》一书中关于伪代码的种种约定：

* 缩进表示块结构；
* `while`、`for` 与 `repeat-until` 等循环结构以及 `if-else` 等条件结构与 C、C++、Java、Python 和 Pascal 中的那些结构具有类似的解释；
* 符号 `//` 表示该行后面部分是个注释；
* 形如 `i = j = e` 的**多重赋值**，它等价于赋值 `j = e` 后跟着赋值 `i = j`；
* 变量（如 `i`、`j` 和 `key`）是**局部于**给定过程的，若无显式说明，我们不使用全局变量；
* 数组元素通过 `“数组名[下标]”` 这样的形式来访问。记号 `..` 用于表示数组中值的一个范围，这样，`A[1..j]` 表示 A 的一个子数组，它包含 `j` 个元素 `A[1], A[2], ..., A[j]`。另外与 JS 不同，伪代码中数组索引从 1 开始；
* 复合数据通常被组织成**对象**，对象又由属性组成。和 JS 类似，我们把一个数组或对象的变量看作指向数据的一个**指针**。所以，伪代码中变量的赋值也类似 JS 中变量赋值，要注意区分基本类型和引用类型；
* 就如同 JS 中给函数参数传值，我们**按值**把伪代码中的参数传递给过程。被调用过程接收其参数自身的副本；
* 一个 `return` 语句立即将控制返回到调用过程的调用点。与许多编程语言不同，我们允许在单一的 `return` 语句中返回多个值；
* 布尔运算符 `and` 和 `or` 都是**短路操作**。这也与 JS 一致，比如 `x and y`，先求值 `x`，如果 `x` 是 `FALSE`，那么表达式已经有了结果，就不会继续对 `y` 求值了；
* 关键词 `error` 表示过程中出现了错误。由调用过程负责处理该错误就好，我们不必在伪代码中说明应对错误具体要采取什么行动。

## （四）算法的运行时间

接下来我们关注一下运行时间的问题。

算法的运行时间取决于诸多因素。其中一个因素是**输入本身的特征**。

拿插入排序来说，如果输入已经有序，那么插入排序所要做的工作就很少了。每次循环要做的操作很少，因为它们已经各就各位了。

Q：那么请思考，什么情况下对于插入排序来说是最坏的情况？
A：逆序情况。这种情况下经过每次循环，不得不把所有元素都整理一遍

### 输入规模

除此之外，算法的运行时间还取决于**输入规模**。

上例中，对于数组 `A = <8, 2, 4, 9, 3, 6>`，我们要处理 6 个数组元素，但如果要处理 60 亿个元素的话，它就需要花上超级长的时间。这就说明，要处理的输入规模越大，所花费的运行时间也就越长。

输入规模依赖于研究的问题。

如果是排序或计算傅里叶变换，最自然的量度是输入中的项数，例如，待排序数组的规模 `n`。

如果是两个整数相乘，输入规模的最佳量度是二进制记号表示输入所需的总位数。

如果某个算法的输入是一个图，那输入规模可以用图中的顶点数和边数来描述，此时用两个数来描述输入规模更合适。

对于研究的每个问题，我们将指出所使用的的「输入规模量度」。

### 运行时间

一个算法在特定输入上的「运行时间」是指执行的基本操作数或步数。

通常，我们处理输入规模的方式是：依输入的规模将其参数化。以后，我们会把运行时间看作待排列数据规模的函数（time as a function of the size of things that we are sorting）。这样，我们在审视算法的行为时就有依据了。

关于运行时间，一般来说，我们都想知道**运行时间的上界**，也就是说我们想知道运行时间是不会超过某个特定量的。

原因在于，**这代表了对用户的一种承诺。**如果我说，这个程序不会运行超过某个时间（比如 3 秒），这就给你了一个关于你该如何用这段程序的真实信息。反之，如果我跟你说这段程序至少要运行 3 秒，你就拿不准它会不会运行 3 秒还是 3 年了......

小结一下，算法的运行时间，和输入有关，取决于 2 点：

1. 输入本身的特征；
2. 输入规模；

另外我们一般关注算法运行时间的上界。

## （五）算法分析，都分析些什么？

人们会对算法做各种各样的分析。我们最关注的一种分析，就是所谓的**最坏情况分析（Worst-Case Analysis）**

我们定义 `T(n)` 为输入规模为 `n` 时的最长运行时间：

`T(n) = maximum time on any input of size n`

它是输入规模为 `n` 时，程序运行的最长可能消耗时间。这意味着，输入有好有坏，我们所关注的，正是所有输入情况中最坏的那种情况。因为只有面对最坏情况，我们才能做出承诺——这种算法总能做到这样，而不是有时能有时又不能。

还有需要注意，如果我们得不到运行时间的最大值，那么 `T(n)` 只能算一种相关性，而不能严格来说作为函数。

有时，我们也会讨论**平均情况（Average-Case）**。

这种情况下，`T(n)` 就成了输入规模 `n` 之下所有可能输入的期望时间：

`T(n) = expected time over all inputs of size n`

Q1：那么什么又是**期望时间（expected time）**呢？
A1：一种回答是，把所有输入的运行时间加起来然后求平均值。这有些靠谱了，接近我们要说的期望时间，但还可以更加严谨。
A2：最严谨答案是，**期望时间，是每种输入的运行时间，乘以那种输入情况出现的概率——也就是一种加权平均**。

Q2：那么问题来了，我们又如何知道每种输入出现的概率是多少？
Q3：我们当然不知道，我们怎么可能知道。所以得做出**假设**，那么要做什么样的假设？在这个假设中需要满足些什么条件呢？

我们需要**有关输入的统计分布（statistical distribution）**的假设。否则的话，期望时间就无从谈起，因为我们不清楚某种输入的可能性是怎么样的。因此为了计算概率，就得做一些假设，并且，还需要将这些假设说明得一清二楚。

最常见的假设之一，就是**所有输入都是以等可能的方式出现的，即「均匀分布」**。

最后我们谈谈**最好情况分析（Best-Case Analysis）**，说白了，可以称这种情况是假象。

为什么说最好情况分析是假象？因为实际场景下要使用算法的地方大多都不是已经有了最好的输入情况。

比如对于排序问题，往往要排序的对象已经已某种方式排序过了，比如银行的支票号码，它们被送入的顺序往往与被签写的顺序相近，它们就是差不多已经排好了的待排序对象，但肯定没法确保它们已经完全排序好了。

所以，如果我们只讨论一个算法的最好情况，那无疑是在诈骗......

**小结**：

对于各种各样的算法，我们**从 3 种情况进行分析**：

1. **最坏情况分析**
2. **平均情况分析**
3. **最好情况分析**

其中，**最坏情况分析是我们最需要关注的，因为这意味着对使用该算法的用户的承诺**。

另外，我们讨论了「期望时间」的概念：**期望时间，是每种输入的运行时间，乘以那种输入情况出现的概率——也就是一种加权平均**。

**但是输入情况的出现概率我们也无从得知，所以只能做「假设」。最常见的假设之一，就是「均匀分布」，即所有输入都是以等可能的方式出现的**。

## （六）渐进分析

那么，插入排序的最坏情况时间是多少？

那么，有趣的事情来了，首先，这取决于执行这个算法的机器怎么样。是个大型超级计算机？还是仅仅是一个腕表芯片？它们的计算能力大相径庭。

所以，当我们比较算法时，**通常比较的是相对速度（relative speed）**，也就是两个算法在同一台机器上的表现。

当然，我也可能对其**绝对速度（absolute speed）**感兴趣，真有某种算法能无论在什么机器上运行都表现良好吗？

对于这 2 种速度：

* 相对速度
* 绝对速度

就要谈到算法的**大局观（big idea）**了。这也是为什么算法涉猎如此广泛，以及为什么算法可以催生像 Google、Akamai 和 Amzon 这样的巨头，为什么算法分析在整个计算史上取得过如此辉煌的成功，为何我们可以掌握并对付那些看起来是一团迷局、复杂无比的局面，最终把它们简化到可以采用一些数学手段处理的程度。那就是**渐进分析**。

**渐进分析的基本思路**：

1. 忽略掉那些依赖于机器的常量（machine-dependent constants）；
2. 不去检查实际的运行时间，而是关注**运行时间的增长（the growth of the running time）**。这里我把查尔斯老师的黑板笔记写出来，我觉得更形象：`Look at growth of T(n) as n -> ∞`

我们重点来看下第 2 点是什么意思，这种思路很伟大，但并不难理解（查尔斯老师幸灾乐祸地说“不然的话我也不可能在第一讲就拿出来教给你们”）。我们要花费数讲的时间来理解它的内涵，而且本质上说，我们整个课程就是在做这么一件事。而且你成为一线工程师的话，成天就干这个。

为了做这件事，就要采用一些有助于我们理解的**符号**，尤其是，我们要采用**渐进符号（Asymptotic notation）**：

### `θ`（theta）渐进符号的使用

掌握它很简单，要做的就是写个公式，弃掉低阶项，并忽略前面的常数因子（drop low order terms and ignore leading constants）。

例如，公式：`3n^3 + 90n^2 - 5n + 6046`

思考哪些是要弃掉的低阶项？

`n^3` 比 `n^2` 阶数高，`n^2` 又比 `n` 阶数高，所以把这些低阶的都舍弃掉，把前面的常数因子也忽略掉，由此得到：

`3n^3 + 90n^2 - 5n + 6046 = θ(n^3)`

是不是很简单？现在，操纵 `θ` 符号已经有了工程方法了，其实它有个数学定义我们后面再说，相当于一种函数定义。

所以对于算法而言，这既是一门数学课，又是一门计算机科学工程课，在整个课程中，咱们得担负着双重责任：既要像对待数学那样有数学的严谨，又得像工程课那样富于直觉，我们两者都要满足。

接着，如果我们关注当 `n` 趋向于无穷大的情况，那么一个 `θ(n^2)` 的算法迟早会战胜一个 `θ(n^3)` 的算法。

也就是说，假如有一个 `θ(n^2)` 的算法，那么总会有一个充分大的 `n`，使得它比 `θ(n^3)` 的算法快，无论那些低阶项是什么都影响不了这个结果。无论常数项是多少也不会动摇这个结果。即使你在一台慢速计算机上运行这个 `θ(n^2)` 算法，而在一台高速计算机上运行这个 `θ(n^3)` 算法，也是一样的结果。

### `θ`（theta）渐进符号的意义

所以，渐进符号 `θ` 的伟大之处在于，**它能一举满足我们对相对和绝对速度的双重比较要求**。在不同的计算平台上，我们也许只是差个常数因子（机器上与运行时间相关的常数因子），但如果我们关注随着输入规模变大的时间增长，渐进结果不会改变。

如果用图来表示：

一条线表示 `θ(n^3)` 的算法，另一条线表示 `θ(n^2)` 的算法，那么总会有某个点 `n0`，比 `n0` 大的任何数，`θ(n^2)` 算法都比 `θ(n^3)` 算法有更小的开销。不管一开始给 `θ(n^3)` 算法多大的优势，在计算机的运行速度等方面给再大优势都没用。

不过如果以工程视角来看，一些问题还是得解决，因为有时候，这样的 `n0` 确实是过大了，大到计算机无法运行该算法，这就是为什么**我们有时会对一些低速的算法感兴趣**。因为**有些低速算法尽管用渐进的观点来看它们较慢，但它们仍然可以在合理规模的输入下运行更快**。

因此，我们必须**在数学理解和工程直觉之间做好权衡，这样才能写出好用的程序**。所以，仅仅会做算法分析，并不能使你自动地成为一个编程高手，你还要学习怎样编程以及在实践中运用这些工具。这样才能了解什么时候它们相关，什么时候不相关。

有一种说法，如果你想成为一个编程高手，只要两年中每天坚持编程，你就能成为编程高手。但如果你想成为一名世界级的大牛，你既可以十年如一日每天坚持编程，也可以两年中每天编程，然后上一门算法课 :)

### 插入排序的最坏情况

言归正传，继续做插入排序的分析。正如之前所说，逆向排好序的情形是插入排序的最坏情况，也就是最大元素占首，最小元素断后，这时每次做插入操作总要把所有项翻一遍。

我们依照循环嵌套的情况，试着写出运行时间。我们要做的就是**求和**，假设算法中每一种原子操作（原子操作就是不能再拆分成更小的操作的操作）都耗费某常数时间，这个常数是多少都无关紧要，因为我们做的是渐进分析，这会使诸多彼此各异的麻烦一举消失，我们就像是从三万英尺的高度看整件事。

回顾之前算法的内层循环：

```js
while i > 0 and A[i] > key
  A[i + 1] = A[i]
  i = i - 1
```

循环内的每项操作都是原子操作。我们对操作进行计数的办法，叫做**内存引用计数，也就是数一数程序访问了内存中的变量多少次**。

拉通整个内外循环（注意这次看算法，重点看循环）：

```js
INSERTION-SORT(A)
  for j = 2 to A.length
    key = A[j]

    i = j - 1
    while i > 0 and A[i] > key
      A[i + 1] = A[i]
      i = i - 1
    A[i+1] = key
```

对于外层循环，`j` 从 2 循环到 `n`，我们把循环里的所有工作全部汇总，就可以写成数学里的求和符号：![j 从 2 到 n 求和](https://latex.oncodecogs.com/gif.latex?\\sum_{j=2}^n)

然后，外层循环体中做了哪些工作呢？

循环体中的工作量有各种可能，我们考虑最坏的情况，对于 `j` 的每一个取值，循环会做多少次操作呢？怎么用渐进的语言表示？

答案是 `θ(j)`。

因为在内存循环中，`i` 以 `j - 1` 作为初始值，然后在每次 `i` 取新值前都只做固定量的工作，也就是 `A[i + 1] = A[i]`。最终 `i` 由 `j - 1` 递减至不再满足循环条件。

所以得到公式：![T(n)的表示](https://latex.codecogs.com/png.image?\dpi{110}%20T(n)%20=%20\sum_{j=2}^{n}\Theta(j))

这就相当于对连续整数求和（数学术语叫「算数级数（arithmetic series），也叫等差级数」），也就是 `2 + 3 + 4 + ... + n`。

因此，这个公式可以简化为：

![T(n)的简化表示](https://latex.codecogs.com/png.image?\dpi{110}%20T(n)%20=%20\sum_{j=2}^{n}\Theta(j)%20=%20\Theta(n^{2}))

## （七）归并排序

有了上面的公式，那插入排序到底快不快呢？

对于很小的 `n`，它的确挺快的。

但对于巨大的 `n` 就完全不是那么回事了。

所以，我们需要一个更快的排序算法，叫「**归并排序（merge sort）**」

如果要对一个数组 `A[1..n]` 做归并排序，那么它有 **3 个基本步骤**：

1. 如果 `n` 是 1，排序完成。因为这种情况下就 1 个数组元素；
2. 递归地对 `A[1 到 n/2 向上取整]` 这部分以及 `A[n/2 向上取整 + 1 到 n]` 这部分排序。因此输入是被分成两半的；
3. 将排好序的两个列表归并，为此，还要用到一个「归并子程序（merge subroutine）」；

### 归并子程序

归并子程序是这样进行归并的，假定有 2 个按反顺序排序后的列表：

* 列表 1：20, 13, 7, 2。相当于 `A[1..n/2]` 这部分；
* 列表 2：12, 11, 9, 1。相当于 `A[n/2 + 1..n]` 这部分。

如图：

要想归并这两个列表，我们就得从这两者出发，最终得到一个排序好的总列表。

先观察，在这 2 个已排序的列表中，最小元素是多少？——> 1

它只可能在 2 个地方出现：要么是列表 1 之首，要么就是列表 2 之首。如图：

找到 `1` 之后，把它输出到最终的数组中，然后把 `1` 叉掉，下一个最小的元素在哪？答案仍在两个列表的首元素之一。再次找到 `2`，输出到最终数组。如图：

以此类推，最终得到输出数组。但巧妙的是，这里面**每一步比较都是固定数目的操作**，这和每一步中的数组尺寸无关。每一步中，只关注 2 个元素，并挑出最小的，接着再把数组指针推进一位，所以总是能知道列表头在哪。

由此，这种情况下，对于总数为 `n` 的输入，算法所花费的时间是 `θ(n)`：![归并算法的 T(n)](https://latex.codecogs.com/png.image?\dpi{110}%20T(n)%20=%20\Theta(n))

我们也将 `θ(n)` 称为「线性时间（linear time）」，因为它不是平方的或是其他怎样，它与 `n` 成正比，即**与输入规模成正比**，这就是线性时间。

### 归并算法的运行时间

前面归并排序的基本步骤中提到，这是个递归程序，所以我们可以为此写出一个递归式。

比如，`n` 个元素的排序时间是 `T(n)`，

那么第一步，“如果 `n` 是 1，排序完成。因为这种情况下就 1 个数组元素；”，要用多长时间？

——答案是只用了常数时间。只需要判断 `n` 是否为 1，如果是则返回。这一步与输入规模毫无关系，因为输入规模固定是 1，它只耗费固定数目的机器指令。无论什么机器都一样，都是**常数时间**，称之为 `θ(1)`

接着，第二步，要递归地对 2 个列表排序：“递归地对 `A[1 到 n/2 向上取整]` 这部分以及 `A[n/2 向上取整 + 1 到 n]` 这部分排序”，这该怎么描述呢？

这时，可以递归地描述成：`T(n/2) + T(n - n/2)`，如果忽略细节，就可以写成 `2T(n/2)`。能这样省略，简直是算法的一大妙处，只要你严格而精确，你可以任意略去细节 :)

这种略去，是因为我们不担心发生意外，因为到头来是没区别的。

最后第三步，我们要归并已排序的、总元素数为 `n` 的两个列表，刚刚用归并子程序分析过，这将消耗 `θ(n)` 的时间。

我把这 3 步对应的运行时间整理如图：

由此，可以为归并排序的性能写出递归表达式：

![归并排序的递归表达式](https://latex.codecogs.com/png.image?\dpi{110}%20T(n)%20=%20\left\{\begin{matrix}\Theta(1),%20&%20n%20=%201%20\\2T(n/2)%20+%20\Theta(n),%20&%20n%20%3E%201%20\\\end{matrix}\right.)

看上方的式子。执行归并算法，要么第一步执行完就返回，要么就是第一步没返回，接着做二三两步。

如果三步都执行，那么总时间就是 `2T(n/2) + θ(n) + θ(1)`，但 `θ(n) + θ(1)` 就是 `θ(n)`，因为 `θ(1)` 比 `θ(n)` 低阶，可以直接舍弃。

所以总的来说，归并算法的执行时间，要么就是 `θ(1)`，要么就是 `2T(n/2) + θ(n)`。

然而在算法层面，我们通常会省略掉 `n = 1` 这种常数规模输入的情况，因为常数规模的输入意味着它也只需要常数的运行时间，而且这对于递归的渐进解结果也没有影响。

### 进一步求解递归式

我们继续进一步求解上面的递归式，现在有个情况是：用 `T(n/2)` 表示了 `T(n)`。

卖个关子，后续这一步将在下篇文章中详解。

但现在先给出一个直观的结论，来理解它所消耗的时间成本。下篇文章中要重点分析的这个方法，就是「**递归树方法（recursion tree technique）**」。

我们在递归式上应用该方法，就会得到 `T(n)` 基本上等于 `2T(n/2)` 后面加上某个大于 0 的常数 `c` 乘以 `n`：

`T(n) = 2T(n/2) + cn`

对比递归式中 `n > 1 时，T(n) = 2T(n/2) + θ(n)`，就会发现，这里用显式的 `cn` 替换掉了隐式的 `θ(n)`，这是因为 `θ(n)` 是线性的，和 `cn` 等价，所以可以这样表示。

接着，构造递归树的方法如下
